# Individual Project Data to Model Pipeline (IPDMP)

## Description

The Individual Project Data to Model Pipeline (IPDMP) is a Python-based project designed to automate the workflow of generating labelled training data from satellite imagery and utilizing this data to train and deploy a machine learning model for water body identification.

The pipeline consists of several key components:

1.  **Navigable Automated Labelling Interface for Regions of Attention (NALIRA)** (`NALIRA.py`): Processes Sentinel-2 satellite imagery, calculates various water indices (NDWI, MNDWI, AWEI-SH, AWEI-NSH), performs cloud masking, and provides an interactive graphical user interface (GUI) for manually labelling regions of interest (reservoirs, water bodies, land). It generates the initial labelled coordinates and segments the image data into smaller classified images (mini-chunks) used for training.
2.  **Keras Reservoir Identification Sequential Platform (KRISP) - Trainer** (`KRISP_trainer.py`, `epoch_pathfinder.py`): Uses the segmented image data generated by NALIRA to train a Keras Sequential convolutional neural network (CNN). It includes a helper script (`epoch_pathfinder.py`) to determine the optimal number of training epochs to prevent overfitting.
3.  **KRISP - Predictor (KRISP-Y)** (`KRISP-Y.py`, `KRISP.py`): Loads the trained KRISP model and deploys it to classify mini-chunks across an entire satellite image tile, generating a prediction file.
4.  **Supporting Modules**: Includes scripts for data handling (`data_handling.py`), image processing (`image_handling.py`), user interfacing elements (`user_interfacing.py`), miscellaneous utilities (`misc.py`), and accuracy assessment (`confusion_matrix.py`).

The overall goal is to provide a streamlined process from raw satellite data to a trained model capable of identifying water features.

## Features

* **End-to-End Pipeline:** Covers data preparation, labelling, training, and prediction.
* **Sentinel-2 Focused:** Optimized for processing Sentinel-2 `.SAFE` format imagery.
* **Water Index Calculation:** Implements standard water indices (NDWI, MNDWI, AWEI-SH, AWEI-NSH).
* **Cloud Masking:** Uses Sentinel-2 quality indicators (QI\_DATA) to mask clouds.
* **Interactive Labelling GUI:** Tkinter-based interface (`NALIRA.py`) for drawing bounding boxes around features of interest (reservoirs, water bodies).
* **Automated Data Segmentation:** `NALIRA.py` automatically crops and saves labelled image examples into class-specific folders.
* **CNN Model Training:** `KRISP_trainer.py` trains a sequential CNN model using Tensorflow/Keras.
* **Epoch Optimization:** `epoch_pathfinder.py` helps visualize training/validation curves to select an appropriate number of epochs.
* **Batch Prediction:** `KRISP-Y.py` efficiently processes large images in batches for prediction.
* **Modular Code:** Organized into distinct Python scripts for different functionalities.

## Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/your-username/IPDMP.git](https://github.com/your-username/IPDMP.git) # Replace with your actual repo URL
    cd IPDMP
    ```

2.  **Install Required Libraries:**
    This project requires Python (tested with 3.x) and several libraries. You can install them using pip:
    ```bash
    pip install numpy matplotlib Pillow tensorflow pandas tqdm scikit-learn # Add any other specific libraries if needed
    ```
    * `numpy`: For numerical operations and array handling.
    * `matplotlib`: For plotting graphs (used in `epoch_pathfinder.py`, `KRISP_trainer.py`, etc.).
    * `Pillow`: For image manipulation (opening, saving, resizing).
    * `tensorflow`: The core machine learning library for Keras model training and prediction.
    * `pandas`: Used in `epoch_pathfinder.py` for results handling.
    * `tqdm`: Used in `epoch_pathfinder.py` for progress bars.
    * `tkinter`: Usually included with Python standard library, used for the GUI in `NALIRA.py`.
    * `scikit-learn`: Often used with TensorFlow/Keras, may be implicitly required or useful for further analysis.

## Usage Instructions

To run the entire pipeline, follow these steps sequentially. This involves generating training data, training the model, deploying the model for prediction, and optionally assessing accuracy.

**Assumptions:**

* You have downloaded the IPDMP code (e.g., as a ZIP file and extracted it).
* You have downloaded a Sentinel-2 satellite image tile in `.SAFE` format. For simplicity, these instructions assume you are using the image: `S2C_MSIL2A_20250301T111031_N0511_R137_T31UCU_20250301T152054.SAFE`.

**1. Setup File Structure**

* **Extract Code:** Extract the downloaded code ZIP file to a known location (e.g., `C:/Users/YourUser/Documents/IPDMP_Code`).
* **Extract Image:** Create a parent folder named `Sentinel 2` in a separate location (e.g., `C:/Users/YourUser/Documents/Satellite_Data/Sentinel 2`). Extract the downloaded satellite image `.SAFE` folder into this `Sentinel 2` folder.
    * **Important:** The image folder (`S2C_MSIL2A...SAFE`) should *not* be inside the code folder, nor should the code folder be inside the image folder. They must be separate, but accessible from a common parent directory or known paths.
* **Create `training data` Folder:** Inside the extracted image folder (e.g., `.../Sentinel 2/S2C_MSIL2A...SAFE/`), create a new folder named `training data`.
* **Move `responses` File:** Navigate to the `responses_5000_chunks.csv` file within your extracted *code* folder. Move this file into the newly created `training data` folder inside the *image* folder.

**2. Training Data Generation (NALIRA)**

* **Configure `NALIRA.py`:**
    * Open the `NALIRA.py` script in a text editor or IDE.
    * Locate the `General Image and Plot Properties` section near the top.
    * Modify the `HOME` variable to the *parent directory* that contains your `Sentinel 2` folder (e.g., `HOME = "C:/Users/YourUser/Documents/Satellite_Data"`).
    * Ensure the `high_res` variable is set to `True` (for using 10m/20m resolution bands).
    * Ensure the `label_data` variable is set to `True`.
* **Run `NALIRA.py`:**
    * Execute the script from your terminal: `python NALIRA.py`
    * The script will first process the satellite bands and calculate water indices. This may take a few minutes.
    * **Data Labelling GUI:** A Tkinter window will appear displaying image chunks (NDWI, MNDWI, TCI). For each chunk:
        * Enter the number of reservoirs visible in the prompt. If reservoirs exist, draw bounding boxes around them in the TCI chunk window.
        * Enter the number of other water bodies visible. If they exist, draw bounding boxes around them.
        * The GUI will guide you on how many ROIs (Regions of Interest) are left to draw for each category. Use the 'Overwrite' button if you make a mistake on the last drawn box. Use 'Select Entire Frame' if the whole chunk represents a single category (e.g., open water). Click 'Finish' *after* drawing all required boxes for the current chunk.
        * Coordinate data is saved automatically to the `responses_5000_chunks.csv` file.
    * **Continue or Break:** Continue labelling chunks as needed. When you are ready to stop labelling and proceed to the next step, type `break` into the console prompt where you entered the number of reservoirs/water bodies.
* **Automated Segmentation:** After breaking from the labelling loop, NALIRA will automatically read the coordinates from the `responses_5000_chunks.csv` file. It will create subfolders within the `training data` folder (e.g., `training data/ndwi/reservoirs`, `training data/ndwi/water bodies`, `training data/ndwi/land`, and similarly for `tci`). It will then crop mini-chunks based on your labelled coordinates and save them as PNG images in the corresponding class folders. This step generates the actual image files used for training KRISP.

**3. Model Training (KRISP Trainer & Epoch Pathfinder)**

* **Determine Optimal Epochs (`epoch_pathfinder.py`):**
    * Open `epoch_pathfinder.py`.
    * Modify the `BASE_PROJECT_DIR` variable to point to the directory containing your `Sentinel 2` folder (e.g., `BASE_PROJECT_DIR = "C:/Users/YourUser/Documents/Satellite_Data"`). Ensure `SENTINEL_FOLDER` matches your image folder name.
    * Verify the `DATA_DIR_NAME` (e.g., `"ndwi"`) matches the subfolder within `training data` containing the images you want to train on.
    * Run the script: `python epoch_pathfinder.py`
    * This script trains the model multiple times with different numbers of epochs (defined in `EPOCH_SETTINGS`). It will output plots showing Training/Validation Accuracy and Loss vs. Number of Epochs.
    * Examine the plots. Choose the highest number of epochs *before* the validation accuracy starts to consistently decrease or flatten while the training accuracy continues to increase (or before validation loss starts to consistently increase). This indicates the onset of overfitting. A value around 100 epochs might be typical, but observe your specific results. Note this number.
* **Train the Final Model (`KRISP_trainer.py`):**
    * Open `KRISP_trainer.py`.
    * Modify the `BASE_PROJECT_DIR` variable as you did for `epoch_pathfinder.py`. Ensure `SENTINEL_FOLDER` and `DATA_DIR_NAME` are correct.
    * Set the `EPOCHS` variable to the optimal number you determined using `epoch_pathfinder.py`.
    * Set `SAVE_MODEL = True` to ensure the trained model is saved.
    * Ensure the `MODEL_FILENAME` reflects the chosen number of epochs (e.g., `MODEL_FILENAME = f"{MODEL_TYPE} model epochs-100.keras"` if you chose 100 epochs).
    * Run the script: `python KRISP_trainer.py`
    * This will train the model using all the training data for the specified number of epochs and save the final model (e.g., `ndwi model epochs-100.keras`) into a `saved_models` folder (which will be created in the `BASE_PROJECT_DIR` if it doesn't exist).

**4. Model Deployment/Prediction (KRISP-Y)**

* **Configure `KRISP-Y.py`:**
    * Open `KRISP-Y.py`.
    * Modify the `HOME` variable to the *parent directory* that contains your `Sentinel 2` folder (e.g., `HOME = "C:/Users/YourUser/Documents/Satellite_Data"`).
    * Set the `folder` variable to the exact name of your Sentinel-2 image `.SAFE` folder.
    * Set the `model_epochs` variable to the *exact* number of epochs used to train the saved model in the previous step (e.g., `model_epochs = 100`). This allows the script to load the correct `.keras` file from the `saved_models` directory.
* **Run `KRISP-Y.py`:**
    * Execute the script: `python KRISP-Y.py`
    * This script will:
        * Load the specified trained model.
        * (If necessary) Generate temporary mini-chunk images from the *entire* satellite tile specified in `folder` (this happens if the required `test data` subfolder doesn't exist or is incomplete). This step can be time-consuming for the first run on an image tile. These test images are saved in a `test data/ndwi_X.XX` folder within the image directory.
        * Run predictions on all the mini-chunks for the tile using the loaded model.
        * Save the predictions to a CSV file named like `P_<n_chunks>_<epochs>_<tile_number>.csv` (e.g., `P_5000_100_T31UCU.csv`) within the satellite image folder. Each row corresponds to a larger chunk, and the columns contain the predicted class for each of the 25 mini-chunks within that larger chunk.

**5. Accuracy Assessment (Optional)**

* The `confusion_matrix.py` script can be used to compare the predictions generated by `KRISP-Y.py` (`P_....csv`) against the ground truth labels created by `NALIRA.py` (`responses_....csv`).
* **Configure `confusion_matrix.py`:**
    * Open the script.
    * Modify the `os.chdir(...)` path to point directly to your specific Sentinel-2 image folder (e.g., `.../Sentinel 2/S2C_MSIL2A...SAFE/`).
    * Ensure the filenames used for `open()` match your responses and predictions CSV files exactly.
* **Run `confusion_matrix.py`:**
    * Execute the script: `python confusion_matrix.py`
    * It will calculate and print metrics like accuracy, precision, sensitivity (recall), and specificity for each class (reservoirs, water bodies, land) based on the comparison of mini-chunk labels.

## Code Structure

* `NALIRA.py`: Main script for data generation and labelling GUI.
* `KRISP_trainer.py`: Trains the Keras CNN model.
* `epoch_pathfinder.py`: Helps determine the optimal number of training epochs.
* `KRISP-Y.py`: Main script for running predictions on full tiles using a trained model.
* `KRISP.py`: Core prediction logic called by `KRISP-Y.py`. Handles model loading, image chunking for prediction, and batch processing.
* `data_handling.py`: Utility functions for CSV manipulation, file path operations, coordinate generation/validation, sorting, etc.
* `image_handling.py`: Functions for loading images, converting to arrays, cloud masking, plotting, saving image chunks/mini-chunks.
* `user_interfacing.py`: Contains functions for the Tkinter ROI selection GUI (`prompt_roi`), console spinners, and table printing.
* `misc.py`: Helper functions for band selection, array splitting, time conversion, user confirmation prompts.
* `confusion_matrix.py`: Calculates accuracy metrics by comparing prediction results to ground truth labels.
* `responses_..._chunks.csv`: (Input/Output) Stores the user-labelled data from NALIRA.
* `P_..._....csv`: (Output) Stores the model predictions from KRISP-Y.
* `saved_models/`: (Output Dir) Folder where trained models (`.keras` files) are saved by `KRISP_trainer.py`.
* `Sentinel 2/<image_folder>/training data/`: (Output Dir) Contains subfolders (`ndwi`, `tci`) which in turn contain class folders (`land`, `reservoirs`, `water bodies`) holding the segmented PNG training images generated by NALIRA.
* `Sentinel 2/<image_folder>/test data/`: (Output Dir) Contains subfolders (e.g., `ndwi_0.41`) holding the temporary mini-chunk PNG images generated by `KRISP.py` for prediction.

## Contributing

1.  Fork the repository.
2.  Create a new branch (`git checkout -b feature/YourFeature`).
3.  Make your changes and commit them (`git commit -m 'Add some feature'`).
4.  Push your changes to your fork (`git push origin feature/YourFeature`).
5.  Submit a pull request.

## Contact

* Nicolas Arroyo, nicolas.renato.arroyo@gmail.com, he/him
